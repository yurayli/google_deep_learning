{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    f = zipfile.ZipFile(filename)\n",
    "    for name in f.namelist():\n",
    "        return tf.compat.as_str(f.read(name))\n",
    "    f.close()\n",
    "\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_unrollings = 10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(labels * -np.log(predictions)) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        #logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        logits = tf.matmul(tf.concat(0, outputs), w) + b\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)  # clip gradients\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)  \n",
    "# note: \n",
    "#   tf.train.optimizer.minimize() same as tf.train.optimizer.compute_gradients() > tf.train.optimizer.apply_gradients()\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])  # one sample character\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297158 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "jbsm cfxterzhnfl iiymuppor ayesidhemtsij dlee a psecpgrpiz onepaicmdaciuc jvwecz\n",
      "oadosic dvjb  u  s l xi x   jin dy xbrfsgutuixdnhzgp kxs naifc esq   aiab hvloei\n",
      "y arvojlg exeni  pj nedtkipptwsy ttkyf taefmkg v enlphbat tgq  coefzpw   rrbkaaa\n",
      "meceoils xvivpdphz  bnwbtbengedpciefazb jkmtdlhiuqjqcowi vxgbaonsehlnedyallsqow \n",
      "g  rrcnm amx jewjfaxyj drz ac khetn qri h i csx wdmsrmcicf auepf totcnqia dm znd\n",
      "================================================================================\n",
      "Validation set perplexity: 20.21\n",
      "Average loss at step 100: 2.585566 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.89\n",
      "Validation set perplexity: 10.43\n",
      "Average loss at step 200: 2.244772 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.51\n",
      "Validation set perplexity: 8.58\n",
      "Average loss at step 300: 2.099000 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 8.19\n",
      "Average loss at step 400: 2.002882 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.65\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 500: 1.939217 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 7.18\n",
      "Average loss at step 600: 1.915412 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 700: 1.864244 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 800: 1.821603 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 900: 1.833316 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.828719 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "ches siguts vay releging be grize apposive of the trade realiculy the to the con\n",
      "ble be suber infofe deatkes reverism preserolive finiotal rehert sideral mrimbat\n",
      "f appeces its and sincree it cloder esiprates of linder folt the is zero cam opl\n",
      "munia of the entery and repucte contration a penyory assolitues the willicul tis\n",
      "reat as befolat recormh is grodent in one nine five five is invilities in ropan \n",
      "================================================================================\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1100: 1.778256 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1200: 1.757111 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1300: 1.733374 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.69\n",
      "Average loss at step 1400: 1.747610 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1500: 1.739647 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1600: 1.749175 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1700: 1.713203 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1800: 1.672844 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 1900: 1.646555 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2000: 1.700299 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "================================================================================\n",
      "ganistarlz polophy polty yoh nec fagn cal augers poputs in the american icunited\n",
      "e of the smatith in the podical actovex to firsting me kn indence seed s statess\n",
      "perens for brovectives that of boak on ut is popences morsline of scodetwe sale \n",
      "zed in vork one nine nine six yead outputel endias as tepbos consode mandear the\n",
      "nmandord assuemsod riftriss argemne a swiak are endlua singet five nine six five\n",
      "================================================================================\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2100: 1.684072 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2200: 1.681918 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 2300: 1.639110 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2400: 1.659047 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 2500: 1.679125 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 2600: 1.650251 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 2700: 1.656288 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2800: 1.648593 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 2900: 1.651749 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3000: 1.652326 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "================================================================================\n",
      "n whol whouthloanio ko nitt secuntine mathudi s full apredes this mard one nine \n",
      "ciating nagiang the transtorrel as monkes papabline and ameay of calked greet ya\n",
      "kanding noted by lian calt was aapon yaxeczs of f meating are freyaldz by time s\n",
      "ficted theoagmon iustource canssible time country one three six alterfom where o\n",
      "qua sestances with used the evengeate white indians of andsaay factal serier of \n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3100: 1.627998 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3200: 1.645358 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3300: 1.635382 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 3400: 1.665860 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 3500: 1.657229 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 3600: 1.671435 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3700: 1.645709 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3800: 1.644275 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3900: 1.638886 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4000: 1.657112 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "hela accelocoll depanse up city where of the kimer to kina polital proded depenc\n",
      "z a churding geosovish he chpole dwision was ordermeter tarrectpl withr mircied \n",
      "rel deiting of full ixe is care was the with over the proceque jap confustionmus\n",
      "entisl canding popularly releate cine to the costences streech from the incompar\n",
      "ic zearqring fielb way noded the haposon full carnaly as an order dothers electe\n",
      "================================================================================\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4100: 1.633246 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4200: 1.635498 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4300: 1.614925 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4400: 1.607345 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 4500: 1.616559 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4600: 1.616076 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4700: 1.625099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4800: 1.632611 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 4900: 1.635367 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5000: 1.609992 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "================================================================================\n",
      "a is rales an four kn colveds of deservand deneli whore the with that one zero t\n",
      " to back die trace the tiknden howet nine two kand randas numbern runch of inter\n",
      "l five one zero zero zero foratw s ikis for be with univers have somilitain his \n",
      "janiment on the the but yall of willioes bah gimes to coajor usen one a proceytr\n",
      "ward his sus or sit a countrie economand from the first could vicol of that acti\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5100: 1.604896 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5200: 1.590004 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5300: 1.579504 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5400: 1.580418 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5500: 1.569140 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5600: 1.579949 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5700: 1.567570 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5800: 1.581301 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5900: 1.573137 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6000: 1.545044 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "h station begandous s from s not the levantings as encoumst agreed bandar subs a\n",
      "il rainor has berdia a bussiote is janckel signies wat febalogiabri more use a j\n",
      "k at chimic food rited guage of highlu infourding did some to gat experedre of t\n",
      "wer in parates one six two zero four five zero zero zero seven zero zero three f\n",
      "z provides one nine do sthictan servication that it a la a number lativer have f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6100: 1.566166 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6200: 1.534260 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6300: 1.543640 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6400: 1.540137 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6500: 1.557904 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6600: 1.592598 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6700: 1.579590 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6800: 1.600655 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6900: 1.575876 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 7000: 1.574261 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "x avation nationination the leas in d egits that two groupally verpnet chargepel\n",
      "it however call of the her thtrats based dramm haluga during dace both poptewial\n",
      "al machole is bking six new was ith sendy one made other but destable of kind wo\n",
      "vie scroment severonito vis that have refore botherie have hamberal dis none wen\n",
      "ing and approne party to the connerding six for other example bnnepe of blited v\n",
      "================================================================================\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 7100: 1.574093 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 7200: 1.569833 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 7300: 1.573632 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.97\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 7400: 1.588467 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 7500: 1.587029 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 7600: 1.555646 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 7700: 1.549248 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 7800: 1.572314 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 7900: 1.579783 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.89\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 8000: 1.617245 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "================================================================================\n",
      "x in the fiction tasserri nor and stybert her difect existly constituted spendin\n",
      "t which accester passamarg cannis eivon an this t the depacutions arrumaidg with\n",
      "me one nine nine zero who put fammes moniednaures from siek alguugg fol three ze\n",
      "erse the move lithtarne musetrible the communicars terp the seta miltroria joxak\n",
      "stos postico was form externatizing troghordes her tirnst mast stale many one ze\n",
      "================================================================================\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 8100: 1.592284 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 8200: 1.567095 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 8300: 1.566694 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 8400: 1.576592 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 8500: 1.582139 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 8600: 1.575780 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 8700: 1.568344 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 8800: 1.541377 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 8900: 1.559591 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 9000: 1.553926 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "ficy macib one nine one legative one wew in the it term of the currently used th\n",
      "x manne from th town i of a are numbers of the e tem complaxers at descractician\n",
      "provenory pyple the firities or internean see usix is afficient served are croms\n",
      "oks strig afredite fighter the one eight seven one eight three one five two zero\n",
      "a that it and recording not and his qued only which seven eight two le within th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.14\n",
      "Elapsed training time: 264.154860973\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "summary_frequency = 100\n",
    "\n",
    "t0 = time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f, learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                    np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                        valid_logprob / valid_size)))\n",
    "print('Elapsed training time:', time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    #ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    #im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    #ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    #fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    #fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    #fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                             \n",
    "    #cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    #cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    #cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    #ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    #om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    #ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # All param variables\n",
    "    wx = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "    wm = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "    wb = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        #input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        #forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        #update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        #state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        #output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        input_output = tf.sigmoid(tf.matmul(i, wx) + tf.matmul(o, wm) + wb)\n",
    "        state = tf.slice(input_output, [0, num_nodes], [tf.shape(i)[0], num_nodes]) * state + \\\n",
    "                    tf.slice(input_output, [0, 0], [tf.shape(i)[0], num_nodes]) * \\\n",
    "                        tf.tanh(tf.slice(input_output, [0, 2*num_nodes], [tf.shape(i)[0], num_nodes]))\n",
    "        output = tf.slice(input_output, [0, 3*num_nodes], [tf.shape(i)[0], num_nodes]) * tf.tanh(state)\n",
    "        return output, state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        #logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        logits = tf.matmul(tf.concat(0, outputs), w) + b\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                logits, tf.concat(0, train_labels)))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)  # clip gradients\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)  \n",
    "# note: tf.train.optimizer.minimize same as tf.train.optimizer.compute_gradients > tf.train.optimizer.apply_gradients\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])  # one sample character\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.311649 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.43\n",
      "================================================================================\n",
      "ces tlvd asy yfevask fipzkb arzr wleoxnpiagtqo   atrxfztx zlss korwkftnrezdbgvng\n",
      "rnlcc s i d a  ftutiae gwarwinhnwe pmhae tf ru ukirmycreneamktoz yel epl n n ef \n",
      "hn ojiooiaanefen evzcvqovssztsufinnjyeaice  pfe k skobtnetxe frgkf ghazxv  unezp\n",
      "dyttgydrnagzns cq kye os    ntlfe n  nznyzf  ty  dharlvts xxwnuygey tv mmpzohoen\n",
      "jite  hidds nqognlpzsan   e ky  edtpzniehm bvfmvjytnonadtnvesltglndshmdoreexthib\n",
      "================================================================================\n",
      "Validation set perplexity: 19.66\n",
      "Average loss at step 100: 2.846731 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.76\n",
      "Validation set perplexity: 14.89\n",
      "Average loss at step 200: 2.656423 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.75\n",
      "Validation set perplexity: 11.79\n",
      "Average loss at step 300: 2.515818 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.36\n",
      "Validation set perplexity: 10.95\n",
      "Average loss at step 400: 2.434738 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.70\n",
      "Validation set perplexity: 10.54\n",
      "Average loss at step 500: 2.387952 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.42\n",
      "Validation set perplexity: 10.50\n",
      "Average loss at step 600: 2.346995 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.22\n",
      "Validation set perplexity: 10.16\n",
      "Average loss at step 700: 2.302595 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.68\n",
      "Validation set perplexity: 10.25\n",
      "Average loss at step 800: 2.262774 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.46\n",
      "Validation set perplexity: 9.97\n",
      "Average loss at step 900: 2.238440 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.82\n",
      "Validation set perplexity: 10.20\n",
      "Average loss at step 1000: 2.218892 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.50\n",
      "================================================================================\n",
      "kced by one be remith ot smevarted two forgasinite pory heor be ige sstiel flio \n",
      "e to zere of t ad a def wo the dops t leats es mof k to codes two tris land co h\n",
      "jtia is thart osee hact the the oculs af ow s mumow tho ercoppol ourof the so br\n",
      "ve al ch dens om fod the ti of oneam tho er efroat wh of mon the ensed it of wal\n",
      "q by jymo wens to freamos lime ung she ever ive sinall miby treis ane suone ttro\n",
      "================================================================================\n",
      "Validation set perplexity: 9.95\n",
      "Average loss at step 1100: 2.183223 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.17\n",
      "Validation set perplexity: 9.79\n",
      "Average loss at step 1200: 2.165187 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.54\n",
      "Validation set perplexity: 8.98\n",
      "Average loss at step 1300: 2.146391 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.96\n",
      "Validation set perplexity: 8.80\n",
      "Average loss at step 1400: 2.127019 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.34\n",
      "Validation set perplexity: 9.17\n",
      "Average loss at step 1500: 2.134578 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.84\n",
      "Validation set perplexity: 9.13\n",
      "Average loss at step 1600: 2.103514 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.72\n",
      "Validation set perplexity: 8.52\n",
      "Average loss at step 1700: 2.101759 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.52\n",
      "Validation set perplexity: 8.80\n",
      "Average loss at step 1800: 2.093036 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.24\n",
      "Validation set perplexity: 8.44\n",
      "Average loss at step 1900: 2.068028 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 2000: 2.061766 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.97\n",
      "================================================================================\n",
      "nagues arepey mand an pingnce buctioncttas at and inatix of fled frouats ozer wi\n",
      "udtuch preters an geotiby pict base fhever the cling on and sist to f ott clanic\n",
      "fingty the cape than filly antersion thile nesgerplechary teotixink frucesly two\n",
      " thee bece statent if the nine ligugiin the svins and six in payted waldy x spoa\n",
      "zoung jopper two crity gepation sedlend wasting bered anther scongate berve fabc\n",
      "================================================================================\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 2100: 2.025078 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 2200: 2.011223 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.47\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 2300: 2.008764 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.74\n",
      "Validation set perplexity: 7.90\n",
      "Average loss at step 2400: 1.974954 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 2500: 2.014800 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 2600: 2.017469 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.26\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 2700: 1.999350 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.49\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 2800: 1.981743 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.84\n",
      "Validation set perplexity: 7.61\n",
      "Average loss at step 2900: 1.981088 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 3000: 1.963521 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "================================================================================\n",
      "cqidal of the tirg have for zero for stam achant hruaters mortide upsuits proit \n",
      "h foved nine semes one pryest extlite the ther thee fcone thot to afrhe tour as \n",
      "ding to this two that the intal womon dewory glaiking one of eight of none zero \n",
      "icling suarlame cand a tereigus an im jadelors aring westration ore sine minked \n",
      "ing pripues nexein to whelivand of iditi frating walling a on thit the wich gein\n",
      "================================================================================\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 3100: 1.944944 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.17\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 3200: 1.946552 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.23\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 3300: 1.958966 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 3400: 1.948731 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 3500: 1.951287 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.57\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 3600: 1.913272 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 7.23\n",
      "Average loss at step 3700: 1.917397 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 3800: 1.931106 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 3900: 1.907613 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.38\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 4000: 1.905629 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "================================================================================\n",
      "ve dideles reh itouns licca pander ecortwendans eight ithil fites to und of two \n",
      "otion and stortomma beonine the in duted is trodaning ktracta dedinated offacks \n",
      "zal asillloa the zartion was silinglivetan abtives theoved mand fove carist gel \n",
      "rited arcen vempiling the dokdly the fing unal tamoit and tomersa withents sticr\n",
      "shater ec to the one stawer esprmaler to the hation ubancivamity that to was cla\n",
      "================================================================================\n",
      "Validation set perplexity: 6.88\n",
      "Average loss at step 4100: 1.891579 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 4200: 1.898424 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 7.12\n",
      "Average loss at step 4300: 1.895738 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.26\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 4400: 1.882386 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 4500: 1.865998 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 4600: 1.906465 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 4700: 1.872545 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 6.84\n",
      "Average loss at step 4800: 1.878481 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 6.85\n",
      "Average loss at step 4900: 1.864127 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.65\n",
      "Average loss at step 5000: 1.878353 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.45\n",
      "================================================================================\n",
      "y loy the evides telbst the chism ack decent wathro purnpurily is the sacts of b\n",
      "x the ench b sacy code gistrumen the end toust os a mangencted mytsu in mbindons\n",
      "plicolled troncils perting ceaterly id the comborth to imbern in clemusia instat\n",
      "fement to for with bfed with will trepan cort ormation of neghy by his the spone\n",
      "x bruberent for the chanter ab igpedst sevarthen and frant mex iszarfing cardina\n",
      "================================================================================\n",
      "Validation set perplexity: 6.66\n",
      "Average loss at step 5100: 1.848170 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 5200: 1.828933 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 5300: 1.813506 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 5400: 1.816099 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 5500: 1.870330 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 5600: 1.840068 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.51\n",
      "Average loss at step 5700: 1.847246 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 5800: 1.851917 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 5900: 1.863638 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 6000: 1.812067 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.00\n",
      "================================================================================\n",
      "nungrast have in aper admoned of nothers or his fealbyegia pradely lisiale the p\n",
      "whenears hy leaives infine penes is dutistegor cetiops of exhia scremay ted jong\n",
      "pleming bapors thow zero apporteral de two zero trepender hes one nine on one tw\n",
      "king costert endart coppusle of farqeion anvive ederile whice in the isasworizie\n",
      "ve pollak loged these wrie haper eight rook maper the bascor the towar bemals de\n",
      "================================================================================\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 6100: 1.805495 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.52\n",
      "Average loss at step 6200: 1.845871 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.49\n",
      "Average loss at step 6300: 1.839310 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 6400: 1.826176 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 6.48\n",
      "Average loss at step 6500: 1.834821 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 6600: 1.823359 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 6.46\n",
      "Average loss at step 6700: 1.856702 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 6800: 1.825076 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.45\n",
      "Average loss at step 6900: 1.839961 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 7000: 1.861188 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.26\n",
      "================================================================================\n",
      "zande two heops os he as stuber to he with the nunmers corim two two eligeburing\n",
      "zice of alriassifle that fith and garestay ubcn free pruacctaber one six zero co\n",
      "tas congorman mase enttenapeht in cluse p and bistor fimic of as were of than la\n",
      "y be fered elauted reluragm scy pears of ausoned ubolive yall was gut himby bocl\n",
      "k the pinks in the wolling salkcridere ap phandems of the that to two macue offl\n",
      "================================================================================\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 7100: 1.846439 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 7200: 1.819867 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 7300: 1.847641 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 7400: 1.831729 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.83\n",
      "Validation set perplexity: 6.41\n",
      "Average loss at step 7500: 1.854465 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 6.39\n",
      "Average loss at step 7600: 1.845271 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 6.42\n",
      "Average loss at step 7700: 1.861593 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 7800: 1.822987 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.32\n",
      "Average loss at step 7900: 1.828247 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.15\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 8000: 1.841636 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.95\n",
      "================================================================================\n",
      "guling evort orked to ave a four tural huse culi ferts which apualanced in insou\n",
      "menthol on lated by influtton sistatars tupeation to wormpustranic frace of they\n",
      "fext by stand a anth five the stensming the gegies monopoance rewo rele one one \n",
      "zen out the poors coidues the voln on landed c two eware a hargies of harkers sw\n",
      "st amo fuly to herican on the chandever bopent mertion three filmed antelled of \n",
      "================================================================================\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 8100: 1.839485 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 8200: 1.837923 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.50\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 8300: 1.820255 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 6.37\n",
      "Average loss at step 8400: 1.830639 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 8500: 1.821720 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 8600: 1.839845 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 8700: 1.836470 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 8800: 1.823417 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 6.33\n",
      "Average loss at step 8900: 1.820356 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 9000: 1.823941 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.42\n",
      "================================================================================\n",
      "m in apbius myrican usedt his maurauther canicappor andly wele bedied inzers pam\n",
      "byticy gorthement one sixpendine and in danvicia sple intriams a the ay the baip\n",
      "e pronectisting the plowednkenign is gen dot be two whergantefour promoulaty sta\n",
      "edine wrow bruccivo the a frodla for humown becring one nine eight presports sun\n",
      "rons by of guneding anvown tyt live oran form the ecowd war has a medn conover d\n",
      "================================================================================\n",
      "Validation set perplexity: 6.33\n",
      "Elapsed training time: 291.868592978\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "summary_frequency = 100\n",
    "\n",
    "t0 = time()\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                    np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                        valid_logprob / valid_size)))\n",
    "print('Elapsed training time:', time()-t0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
